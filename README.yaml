# FPV therealspotter  ğŸ  
**Vision-based FPV race analysis, gate learning, and lap timing**

therealspotter / lazyspoter is a **computer-vision tool for FPV drone racing** that detects gates from video, identifies gate passes, learns track layouts, and produces **lap timing and gate-to-gate performance statistics**.

The project is designed for:
- FPV pilots who want to analyze race footage
- Track builders who want automatic gate ordering
- Dataset creation and model training
- Research and experimentation in vision-based racing analytics

All processing runs **locally on the client** (no cloud), and the project is written entirely in **Python**.

---

## âœ¨ Key Features

- ğŸ¯ Gate detection using YOLO (custom-trained or generic models)
- ğŸ§  Gate re-identification (Re-ID) using CLIP embeddings
- ğŸ—ï¸ **Learn mode** â€“ build a gate order from video
- ğŸ **Race mode** â€“ match passes against a known track
- ğŸ¤– **Auto mode** â€“ freestyle: auto-create or match gates on the fly
- â±ï¸ Lap timing & per-gate timing
- ğŸ§ª Rich debug panels for pass detector & GateDB
- ğŸ·ï¸ Pseudo-label generation for YOLO datasets
- ğŸ–Šï¸ Manual video annotation tools

---

## ğŸ“ Project Structure (high level)

.
â”œâ”€â”€ vocab_spotter.py # Vocabulary-based detector (experimental)
â”œâ”€â”€ lazy_spotter.py # Main FPV gate spotter (recommended)
â”œâ”€â”€ gate_db.py # Gate memory, ordering, laps, race logic
â”œâ”€â”€ pass_detector.py # Vision-only gate pass detection
â”œâ”€â”€ fpv_pseudo_label*.py # Dataset auto-labeling tools
â”œâ”€â”€ visualize_fpvgates.py # Dataset visualization
â”œâ”€â”€ video_labeler_yolo.py # Manual annotator
â”œâ”€â”€ vocab/ # Optional vocabulary embeddings
â””â”€â”€ datastuff/ # Datasets, videos, outputs


---

## ğŸ§° Environment Setup

Work inside a Python virtual environment:

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt



Main Tool â€“ Non-Vocabulary Pipeline (lazy_spotter.py)

This is the primary entrypoint for learning tracks, racing, and freestyle analysis.

ğŸ—ï¸ LEARN mode â€“ build a track layout

Creates an ordered gate memory from video.

python lazy_spotter.py \
  --mode learn \
  --det-model current_best_non_vocab.pt \
  --video ~/Downloads/1/Kfar\ Hess\ disaster.mp4 \
  --pass-enable \
  --save-pass-crops \
  --gate-memory gate_memory.json


Gates are detected
Passes are detected visually
You confirm / skip gates
Multiple embeddings per gate are collected
Track order is learned


RACE mode â€“ analyze a known track

Matches detected passes against a previously learned track.

python lazy_spotter.py \
  --mode race \
  --det-model current_best_non_vocab.pt \
  --video ~/Downloads/1/Kfar\ Hess\ disaster.mp4 \
  --pass-enable \
  --save-pass-crops \
  --gate-memory gate_memory.json

Gate-by-gate matching
Lap detection
Lap times
Per-gate timing statistics

AUTO mode â€“ freestyle / unknown tracks

Automatically creates or matches gates without enforcing order.

python lazy_spotter.py \
  --mode auto \
  --det-model current_best_non_vocab.pt \
  --video datastuff/fpv_videos/test/ROSH\ HAAYIN\ WITH\ GQ.mp4 \
  --pass-enable \
  --save-pass-crops


Debug/Viz tunings:
ENABLE_GATE_DB = True          # Enable GateDB logic
GATE_ID_VIZ = True             # Show GateID hints on screen
DEBUG_PASS_PANEL_VIZ = True    # Pass detector debug window
SAVE_PASS_CROPS_TODISK = True  # Save CLIP embedding crops
GATE_DB_RACE_STATS = True      # Race statistics panel

Tunings:
These are the most impactful parameters for pass detection quality
(found in pass_detector.py):

aligned_shrink_reset_frac	- Sensitivity to gates that are already aligned but moving away
pass_cooldown_sec	 - Cooldown between gate passes (depends on pilot speed)
aligned_max_age_sec	 - How long a gate may remain aligned without passing (often up to ~10s)



-------------------------
Training a YOLO Detector

#sanity only
python pipeline_sanity_train_eval.py \
  --data-root ./datastuff/fpv_dataset \
  --data-yaml ./fpv_gate.yaml \
  --nc 4 \
  --print-paths

#sanity + train
python pipeline_sanity_train_eval.py \
  --data-root ./datastuff/fpv_dataset \
  --data-yaml ./fpv_gate.yaml \
  --nc 4 \
  --train \
  --base-model yolov8m.pt \
  --epochs 20 \
  --imgsz 416 \
  --batch 16 --device mps --name tempdelete \
  --eval-split test \
  --print-paths

#evaluate (mAP and precision/recall scores)
python pipeline_sanity_train_eval.py \
  --data-root ./datastuff/fpv_dataset \
  --data-yaml ./fpv_gate.yaml \
  --nc 4 \
  --weights runs/detect/fpv_gate_more_annot2/weights/best.pt \
  --eval-split test \
  --print-paths

#run just detector
python3 inspect_gate_model_video.py \
  --model runs/detect/train/weights/best.pt \
  --video /path/to/test.mp4 \
  --conf 0.25


#generate yolo dataset from videos (mp4):
python fpv_pseudo_label.py \
  --video_dir ./videos \
  --out_dir ./dataset \
  --yolo_model current_best_non_vocab.pt \
  --conf 0.25 \
  --maxdet 50 \
  --min_area_ratio 0.002

#visualize / manage labels on images:
python visualize_fpvgates.py \ 
  --images_dir datastuff/fpv_dataset/images/train \
  --labels_dir datastuff/fpv_dataset/labels/train \
  --max_images 4000

#manual annotation tool
python video_labeler_yolo.py \ 
  --video ~/Downloads/1/Rosh.mp4 \
  --out ./datastuff/fpv_dataset_labeled \
  --split train \
  --classes "square,arch,circle,flagpole" \
  --start 0 \
  --step 1 \
  --resize 1280x720


  #parameters that i often tune between tracks/video res/detector quality on a set of gates:
  aligned_shrink_reset_frac #how sensitive we are to gate that is already in "aligned" but moving away from it
  pass_cooldown_sec #cooldown between gate passes (per how much the pilot is fast basically)
  aligned_max_age_sec #how long should we keep align as long as its seen, i set it to maximum 10sec for now



OBSOLETE - Vocabulary-Based Spotter (experimental)

Uses predefined CLIP embeddings stored under vocab/.

python vocab_spotter.py \
  --mode learn \
  --video ~/Downloads/1/Rosh.mp4 \
  --vocab ./vocab \
  --device mps \
  --ttl-seconds 0.3 \
  --hide-after 0.1 \
  --max-candidates 4 \
  --det-conf 0.08 \
  --pass-enable


#README
#work inside python3 venv


############## vocab based:
#vocabulary in vocab/

############ non vocab based:




#train the model on mac for Mac M2:

#sanity check on the folders and labels









#--------------------
# utilities
